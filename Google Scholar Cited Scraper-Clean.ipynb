{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installation\n",
    "#!pip install requests --upgrade #upgrades system\n",
    "import requests #We can use requests.get to download a page\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import math\n",
    "import random\n",
    "#We can use the BeautifulSoup class to parse an HTML document.\n",
    "#!pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we need to define headers in this function because google scholar webpage requires a login\n",
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}\n",
    "url = 'https://scholar.google.com/scholar?cites=11660772839861173263&as_sdt=2005&sciodt=0,5&hl=en'\n",
    "response =requests.get(url,headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code # checks to make sure response is going through, should return 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content,'html.parser') #sets up parser object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function for the getting inforamtion of the web page\n",
    "def get_paperinfo(paper_url):\n",
    "\n",
    "  #download the page\n",
    "  response =requests.get(url,headers=headers)\n",
    "\n",
    "  # check successful response\n",
    "  if response.status_code != 200:\n",
    "    print('Status code:', response.status_code)\n",
    "    raise Exception('Failed to fetch web page ')\n",
    "\n",
    "  #parse using beautiful soup\n",
    "  paper_doc = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "  return paper_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = get_paperinfo(url) #getting paper info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping Functions\n",
    "# this function for the extracting information of the tags\n",
    "def get_tags(doc):\n",
    "  paper_tag = doc.select('[data-lid]')\n",
    "  cite_tag = doc.select('[name=\"Cites\"] + a')\n",
    "  link_tag = doc.find_all('h3',{\"class\" : \"gs_rt\"})\n",
    "  author_tag = doc.find_all(\"div\", {\"class\": \"gs_a\"})\n",
    "\n",
    "  return paper_tag,cite_tag,link_tag,author_tag\n",
    "\n",
    "# it will return the title of the paper\n",
    "def get_papertitle(paper_tag):\n",
    "  \n",
    "  paper_names = []\n",
    "  \n",
    "  for tag in paper_tag:\n",
    "    paper_names.append(tag.select('h3')[0].get_text())\n",
    "\n",
    "  return paper_names\n",
    "\n",
    "# function for the getting link information\n",
    "def get_link(link_tag):\n",
    "\n",
    "  links = []\n",
    "\n",
    "  for i in range(len(link_tag)) :\n",
    "    links.append(link_tag[i].a['href']) \n",
    "\n",
    "  return links \n",
    "\n",
    "# it will return the number of citation of the paper\n",
    "def get_citecount(cite_tag):\n",
    "  cite_count = []\n",
    "  for i in cite_tag:\n",
    "    cite = i.text\n",
    "    if i is None or cite is None:  # if paper has no citatation then consider 0\n",
    "      cite_count.append(0)\n",
    "    else:\n",
    "      tmp = re.search(r'\\d+', cite) # its handle the None type object error and re use to remove the string \" cited by \" and return only integer value\n",
    "      if tmp is None :\n",
    "        cite_count.append(0)\n",
    "      else :\n",
    "        cite_count.append(int(tmp.group()))\n",
    "\n",
    "  return cite_count\n",
    "\n",
    "# function for the getting autho , year and publication information\n",
    "def get_author_year_publi_info(authors_tag):\n",
    "  years = []\n",
    "  publication = []\n",
    "  authors = []\n",
    "  for i in range(len(authors_tag)):\n",
    "      authortag_text = (authors_tag[i].text).split()\n",
    "      year = int(re.search(r'\\d+', authors_tag[i].text).group())\n",
    "      years.append(year)\n",
    "      publication.append(authortag_text[-1])\n",
    "      author = authortag_text[0] + ' ' + re.sub(',','', authortag_text[1])\n",
    "      authors.append(author)\n",
    "      #sleep(3)\n",
    "  \n",
    "  return years , publication, authors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get num of results, to iterate through all the pages\n",
    "def get_page_num(doc, incr):\n",
    "    result = doc.find_all( \"div\" , {\"class\": \"gs_ab_mdw\"})\n",
    "    val = result[1].get_text()\n",
    "    num = int(re.search(r'\\d+', val).group())\n",
    "    page = int(incr*math.ceil(num/incr))\n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating final repository\n",
    "paper_repos_dict = {\n",
    "                    'Paper Title' : [],\n",
    "                    'Year' : [],\n",
    "                    'Author' : [],\n",
    "                    #'Citation' : [],\n",
    "                    'Publication' : [],\n",
    "                    'Url of paper' : [] }\n",
    "\n",
    "# adding information in repository\n",
    "\n",
    "def add_in_paper_repo(papername,year,author,publi,link):\n",
    "    paper_repos_dict['Paper Title'].extend(papername)\n",
    "    paper_repos_dict['Year'].extend(year)\n",
    "    paper_repos_dict['Author'].extend(author)\n",
    "    #paper_repos_dict['Citation'].extend(cite)\n",
    "    paper_repos_dict['Publication'].extend(publi)\n",
    "    paper_repos_dict['Url of paper'].extend(link)\n",
    "\n",
    "    return pd.DataFrame(paper_repos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-bce3824dc815>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# add in paper repo dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mfinal22\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_in_paper_repo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaper_title\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauthor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpublication\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mrint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;31m# use sleep to avoid status code 429\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "#last_page_num = get_page_num(paper, 10) #get last page\n",
    "#Scrape Multiple Pages\n",
    "for i in range(0,20,10):\n",
    "    # get url for the each page\n",
    "    url = \"https://scholar.google.com/scholar?start={}&hl=en&as_sdt=2005&sciodt=2006&cites=11660772839861173263&scipsc=\".format(i)\n",
    "    # function for the get content of each page\n",
    "    doc = get_paperinfo(url)\n",
    "\n",
    "    # function for the collecting tags\n",
    "    paper_tag,cite_tag,link_tag,author_tag = get_tags(doc)\n",
    "    \n",
    "    # paper title from each page\n",
    "    paper_title = get_papertitle(paper_tag)\n",
    "    \n",
    "    # year , author , publication of the paper\n",
    "    year , publication , author = get_author_year_publi_info(author_tag)\n",
    "    \n",
    "    # cite count of the paper \n",
    "     #cite = get_citecount(cite_tag)\n",
    "\n",
    "    # url of the paper\n",
    "    link = get_link(link_tag)\n",
    "    \n",
    "    #get iteration:\n",
    "    print(i)\n",
    "    print(\"----------------------------------------------\")\n",
    "    # add in paper repo dict\n",
    "    final = add_in_paper_repo(paper_title,year,author,publication,link)\n",
    "    rint = random.randint(30, 90)\n",
    "    print(rint) #shows rint\n",
    "    # use sleep to avoid status code 429\n",
    "    sleep(rint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Author</th>\n",
       "      <th>Publication</th>\n",
       "      <th>Url of paper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[HTML][HTML] Environmental neuroeconomics: how...</td>\n",
       "      <td>2021</td>\n",
       "      <td>N Sawe</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision neuroscience and neuroeconomics: Rece...</td>\n",
       "      <td>2022</td>\n",
       "      <td>JB Dennison</td>\n",
       "      <td>Library</td>\n",
       "      <td>https://wires.onlinelibrary.wiley.com/doi/abs/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[HTML][HTML] Data Triangulation in Consumer Ne...</td>\n",
       "      <td>2020</td>\n",
       "      <td>CC Cao</td>\n",
       "      <td>frontiersin.org</td>\n",
       "      <td>https://www.frontiersin.org/articles/10.3389/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neuromarketing: How to Choose the Right Measures</td>\n",
       "      <td>2020</td>\n",
       "      <td>S Bellman</td>\n",
       "      <td>taylorfrancis.com</td>\n",
       "      <td>https://www.taylorfrancis.com/chapters/edit/10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neuroscience in service research: an overview ...</td>\n",
       "      <td>2019</td>\n",
       "      <td>N Verhulst</td>\n",
       "      <td>emerald.com</td>\n",
       "      <td>https://www.emerald.com/insight/content/doi/10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[HTML][HTML] Media content sharing as a value-...</td>\n",
       "      <td>2020</td>\n",
       "      <td>C Scholz</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Using neuroimaging to predict behavior: An ove...</td>\n",
       "      <td>2019</td>\n",
       "      <td>SH Tompson</td>\n",
       "      <td>psycnet.apa.org</td>\n",
       "      <td>https://psycnet.apa.org/record/2019-55953-010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Studying volition with actions that matter: Co...</td>\n",
       "      <td>2020</td>\n",
       "      <td>L Mudrik</td>\n",
       "      <td>psycnet.apa.org</td>\n",
       "      <td>https://psycnet.apa.org/buy/2019-76272-001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Neuroscience in service research: an overview ...</td>\n",
       "      <td>2020</td>\n",
       "      <td>A De</td>\n",
       "      <td>biopen.bi.no</td>\n",
       "      <td>https://biopen.bi.no/bi-xmlui/handle/11250/263...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[HTML][HTML] Representation, pattern informati...</td>\n",
       "      <td>2018</td>\n",
       "      <td>PA Kragel</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Paper Title  Year       Author  \\\n",
       "0  [HTML][HTML] Environmental neuroeconomics: how...  2021       N Sawe   \n",
       "1  Decision neuroscience and neuroeconomics: Rece...  2022  JB Dennison   \n",
       "2  [HTML][HTML] Data Triangulation in Consumer Ne...  2020       CC Cao   \n",
       "3   Neuromarketing: How to Choose the Right Measures  2020    S Bellman   \n",
       "4  Neuroscience in service research: an overview ...  2019   N Verhulst   \n",
       "5  [HTML][HTML] Media content sharing as a value-...  2020     C Scholz   \n",
       "6  Using neuroimaging to predict behavior: An ove...  2019   SH Tompson   \n",
       "7  Studying volition with actions that matter: Co...  2020     L Mudrik   \n",
       "8  Neuroscience in service research: an overview ...  2020         A De   \n",
       "9  [HTML][HTML] Representation, pattern informati...  2018    PA Kragel   \n",
       "\n",
       "         Publication                                       Url of paper  \n",
       "0           Elsevier  https://www.sciencedirect.com/science/article/...  \n",
       "1            Library  https://wires.onlinelibrary.wiley.com/doi/abs/...  \n",
       "2    frontiersin.org  https://www.frontiersin.org/articles/10.3389/f...  \n",
       "3  taylorfrancis.com  https://www.taylorfrancis.com/chapters/edit/10...  \n",
       "4        emerald.com  https://www.emerald.com/insight/content/doi/10...  \n",
       "5           Elsevier  https://www.sciencedirect.com/science/article/...  \n",
       "6    psycnet.apa.org      https://psycnet.apa.org/record/2019-55953-010  \n",
       "7    psycnet.apa.org         https://psycnet.apa.org/buy/2019-76272-001  \n",
       "8       biopen.bi.no  https://biopen.bi.no/bi-xmlui/handle/11250/263...  \n",
       "9           Elsevier  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send to csv\n",
    "final.to_csv('aggregate_demand_neuroforecasting10.csv', sep=',', index=False,header=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
